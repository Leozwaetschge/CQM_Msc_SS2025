{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: With the parameters already set (number of sweeps, number of measurements etc) the whole notebook takes dozens of minutes to run. For debugging purpose you can decrease the number of updates and measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import queue\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short introduction to python classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to write a class that performs the Monte Carlo simulations of the classical Ising model. For this, we here provide a short introduction to the basics of python classes. A detailed tutorial on python classes can be found under https://docs.python.org/3/tutorial/classes.html.\n",
    "\n",
    "People that are already familiar with python classes might skip this introduction.\n",
    "\n",
    "Let's consider a simple example class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, l, h):\n",
    "        self.length=l\n",
    "        self.height=h\n",
    "        print(\"Length and height are set to values\",l,\"and\",h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here defined a class that stores the length and height of a rectangle in its member variables self.length and self.height. In particular, the argument 'self' denotes the current object of the class - all member variables are therefore given by 'self.variablename'. In addition, self is passed as argument to each member function.\n",
    "\n",
    "The class we defined above contains one function called __init__. This is a special function that is called automatically, when a new instance of the class is created. Class instantiation uses function notation. We can just pretend that the class object is a function that takes the parameters of __init__ that returns a new instance of the class. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrectangle=Rectangle(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have an instance of the class 'Rectangle' called 'myrectangle', storing a rectangle with length=2 and height=4. We can directly access the member variables via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myrectangle.length,myrectangle.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the class we have written so far is quite boring - the only thing it does is storing length and height of a Rectangle. We thus may add some **member functions**, that compute some properties of the rectangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rectangle:\n",
    "    def __init__(self, l, h):\n",
    "        self.length=l\n",
    "        self.height=h\n",
    "        print(\"Length and height are set to values\",l,\"and\",h)\n",
    "        \n",
    "    def area(self):\n",
    "        return self.length*self.height\n",
    "    \n",
    "    def print_area(self):\n",
    "        print(\"The area of the rectangle is given by \",self.area())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here added a member function that computes the area of the rectangle, and a member function that prints the area of the rectangle by calling the member function area. Note here, that within the class, member function are called via self.functionname.\n",
    "Let us test these functions by defining a new instance of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrectangle2=Rectangle(3.5,2)\n",
    "myrectangle2.print_area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are all set to write a class for the Monte Carlo simulation of the Ising model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation of the classical 2D Ising model\n",
    "## 1) Local updates\n",
    "\n",
    "We want to simulate the classical Hamiltonian $H=-J \\sum \\limits_{\\langle i,j\\rangle} s_i s_j$ via the Metropolis algorithm. We start with an initial spin-configuration, which we can e.g. choose as all spins up. In each step of the Markov chain, a random site $i$ is chosen, and the next configuration is obtained by flipping spin $s_i$ with probability $min[1,e^{-\\frac{\\Delta E}{ T}}]$ ($k_B=1$). We start by writing a class containing the basic steps of the Markov chain. In particular, we will need the following member functions:\n",
    "\n",
    "- **update_probabilities**:\n",
    "A Markov chain is computationally expensive, as typically a large number of steps is required to compute observables with reasonable accuracy. One of the expensive parts lies in computing the exponential $e^{-\\frac{\\Delta E}{T}}$ in each step. Convince yourself that $\\Delta E$ can only take 5 discrete values, $\\Delta E= 2Js_i h_i$, where $h_i =\\sum_{\\langle i, j\\rangle} s_j$. Then, we can reduce the computational cost by defining the member variable self.mrt_prob which consists of the pre-computed exponentials for each of the 5 values that $\\Delta E$ can take. Complete the member function 'update_probabilities' to set mrt_prob to the 5 possible values of $e^{-\\frac{\\Delta E}{T}}$.\n",
    "\n",
    "- **set_temperature**:\n",
    "(Re-)sets the temperature self.T (probabilities mrt_prob need to be updated when T is set or changed!) \n",
    "\n",
    "- **reset_spins**:\n",
    "Sets the spins (array self.spins) to the initial configuration, which we here choose as all spins up. Keep in mind that we want to calculate the magnetization $\\langle |m| \\rangle=|1.0/L^2 \\langle \\sum_i s_i \\rangle|$. For this, it is useful to keep track of the quantity $M=\\sum_i s_i$ (member variable self.M) during the Metropolis algorithm. Set the initial value of $M$ in **reset_spins**.\n",
    "\n",
    "- **mrt_step**: \n",
    "Performs one step of the Markov chain. Keep in mind to update self.M as well.\n",
    "\n",
    "- **mrt_sweep**:\n",
    "Performs one Monte Carlo sweep, consisting of $L*L$ steps of the Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingMC_Metropolis:\n",
    "    def __init__(self, length, temperature=0.):\n",
    "        self.spins = np.ones((length,length),dtype=int) #2D array of spins, corresponds to the current configuration\n",
    "        self.L = length\n",
    "        self.T = temperature\n",
    "        self.M = length * length #magnetization, we start with all spins up\n",
    "        self.mrt_prob = None  #should be set to array of length 8 in update_probabilities.\n",
    "        self.update_probabilities()\n",
    "    \n",
    "   \n",
    "    def update_probabilities(self):\n",
    "        '''we tabularize the probabilities using self.mrt_prob so we don't have to recompute them '''\n",
    "        if(self.T != 0.):\n",
    "            #ising acceptance probabilities\n",
    "            #implement here\n",
    "            pass\n",
    "        else:\n",
    "            #ising acceptance probabilities\n",
    "            #implement here\n",
    "            pass\n",
    "        \n",
    "   \n",
    "    \n",
    "    def set_temperature(self, temperature):\n",
    "        '''set temperature and update the probabilities '''\n",
    "        #implement here\n",
    "        \n",
    "    \n",
    "    def reset_spins(self):\n",
    "        '''this resets the spins to the all-up state '''\n",
    "        #implement here\n",
    "    \n",
    "    def mrt_step(self):\n",
    "        '''performs one update step using single spin Metropolis'''\n",
    "        #implement here\n",
    "    \n",
    "    def mrt_sweep(self):\n",
    "        '''perform an update sweep consisting of L*L steps using single spin Metropolis'''  \n",
    "        #implement here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Thermalization analysis\n",
    "We start in the M=1 state and relax to different temperatures, sampling the order parameter after each Metropolis update. From that sampled data we calculate averages that take a growing number of samples into account. Note that this data is contaminated by the initial values, so we're probably underestimating the relaxation speed. Observing the convergence of these averages, we can reason about the equilibration dynamics at different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "dt = 0.1\n",
    "num_updates = L*L*100\n",
    "\n",
    "sys = IsingMC_Metropolis(L)\n",
    "temperatures = np.arange(0.,5.0,dt)\n",
    "data = []\n",
    "for t in temperatures:\n",
    "    mag_data = []\n",
    "    sys.reset_spins()\n",
    "    sys.set_temperature(t)\n",
    "    for update in range(num_updates):\n",
    "        sys.mrt_step()\n",
    "        mag_data.append(np.abs(sys.M)/(sys.L*sys.L))\n",
    "    data.append(mag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating averages...\")\n",
    "averages = []\n",
    "count = 0\n",
    "for dataset in data:\n",
    "    current = []\n",
    "    for i in range(1,len(dataset),10): \n",
    "        current.append(np.mean(dataset[:i]))\n",
    "    averages.append(np.array(current))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(25, 2, figsize=(15,69))\n",
    "for i in range(25):\n",
    "    axs[i,0].plot(averages[2*i], label=f\"T={temperatures[2*i]}\")\n",
    "    axs[i,0].legend()\n",
    "    axs[i,0].set_ylim([0.,1.05])\n",
    "    axs[i,0].set_ylabel(\"Order parameter estimate\")\n",
    "    axs[i,0].set_xlabel(\"Number of sweeps x 10\")\n",
    "    axs[i,1].plot(averages[2*i+1], label=f\"T={temperatures[2*i+1]}\")\n",
    "    axs[i,1].legend()\n",
    "    axs[i,1].set_ylim([0.,1.05])\n",
    "    axs[i,1].set_xlabel(\"Number of sweeps x 10\")\n",
    "\n",
    "axs[24,0].set_xlabel(\"Average cutoff (step number)\");\n",
    "axs[24,1].set_xlabel(\"Average cutoff (step number)\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretation:\n",
    "Typically, you should find that a good choice is an x-axis value of 200, which corresponds to 20 sweeps.\n",
    "\n",
    "However, we should have a closer look at the critical region, i.e. temperatures between 2.1 and 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "dt = 0.1\n",
    "num_updates = L*L*5000\n",
    "\n",
    "sys = IsingMC_Metropolis(L)\n",
    "temperatures = np.arange(2.1,2.7,dt)\n",
    "data = []\n",
    "for t in temperatures:\n",
    "    mag_data = []\n",
    "    sys.reset_spins()\n",
    "    sys.set_temperature(t)\n",
    "    for update in range(num_updates):\n",
    "        sys.mrt_step()\n",
    "        mag_data.append(np.abs(sys.M)/(sys.L*sys.L))\n",
    "    data.append(mag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating averages...\")\n",
    "averages = []\n",
    "count = 0\n",
    "for dataset in data:\n",
    "    current = []\n",
    "    for i in range(1,len(dataset),500):\n",
    "        current.append(np.mean(dataset[:i]))\n",
    "    averages.append(np.array(current))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(15,9))\n",
    "for i in range(3):\n",
    "    axs[i,0].plot(averages[2*i], label=f\"T={temperatures[2*i]}\")\n",
    "    axs[i,0].legend()\n",
    "    axs[i,0].set_ylabel(\"Order parameter estimate\")\n",
    "    axs[i,0].set_ylim([0.,1.05])\n",
    "    axs[i,0].set_xlabel(\"Number of sweeps / 5\")\n",
    "    axs[i,1].plot(averages[2*i+1], label=f\"T={temperatures[2*i+1]}\")\n",
    "    axs[i,1].legend()\n",
    "    axs[i,1].set_ylim([0.,1.05])\n",
    "    axs[i,1].set_xlabel(\"Number of sweeps / 5\")\n",
    "axs[2,0].set_xlabel(\"Average cutoff (step number)\");\n",
    "axs[2,1].set_xlabel(\"Average cutoff (step number)\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "Close to the critical region, you should find that the system needs a larger number of sweeps to relax (order of 1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simulation\n",
    "We use the above observations to run simulations at different temperatures and calculate the order parameter. We take 2^16 samples, so we can easily bin the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning analysis utilities\n",
    "First we write a function that will perform the binning analysis for us following the scheme in the exercise sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_step(data):\n",
    "    ''' performs a single binning step\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    - data=[Q^(l-1)_1 ... Q^(l-1)_N]\n",
    "      array of length N=2M, containing N measurements Q^(l-1)_i (e.g. magnetization m^(l-1)_i) \n",
    "      in the (l-1)'th level of the binning analysis\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    - new_data=[Q^(l)_1, ... Q^(l)_N]\n",
    "      array of length M (lth level array of the binning analysis)\n",
    "      \n",
    "    - new_error: double\n",
    "      error estimate of lth level of the binning analysis (eq. (6) in exercise sheet)\n",
    "    \n",
    "    '''\n",
    "    #implement here\n",
    "    \n",
    "    return new_data, new_error\n",
    "\n",
    "#bin the data up to num_levels\n",
    "def binning(data, num_levels):\n",
    "    ''' bins the data up to num_level\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    - data=[Q_1 ... Q_N]:\n",
    "      array of length N, containing N measurements Q_i (e.g. magnetization m_i) \n",
    "      \n",
    "    - num_levels: int\n",
    "      number of binning levels to be computed\n",
    "      \n",
    "      \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    - errors: array, dtype=double\n",
    "      array of length num_levels+1, contains error estimates for each level\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    errors = []\n",
    "    #implement here\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation parameters\n",
    "Now we define the parameters of our simulation. We'll use a very high number of samples and measure after each single-spin update. This highlights the properties of the Metropolis algorithm better. Typically you would perform a few uncorrelating updates between successive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperatures at which we sample\n",
    "#finer graining around the critical temperature; this is where the money is!\n",
    "temps_low = np.arange(0.,2.1,0.1)\n",
    "temps_crit = np.arange(2.1,2.7,0.02)\n",
    "temps_high = np.arange(2.8,5.,0.1)\n",
    "temperatures = np.concatenate((temps_low,temps_crit,temps_high))\n",
    "\n",
    "#number of relaxation steps we want to perform\n",
    "def relaxation_sweeps(temperature):\n",
    "    if temperature < 2.1 or temperature > 2.7:\n",
    "        return 20\n",
    "    else:\n",
    "        return 1000\n",
    "\n",
    "#number of samples we take\n",
    "num_samples = 2**20\n",
    "\n",
    "#maximum binning level we consider\n",
    "num_levels = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "Apart from the order parameter, we also measure the actual magnetization, which should be zero for all temperatures. This takes several minutes to run. Implement here the Metropolis algorithm for each temperature including relaxation sweeps in order to equilibrate the system. After thermalization, collect the magnetization and the absolute value of the magnetization. Save the complete collection of the measurements in \n",
    "m_abs_data=[m_abs_data_T_1, ...m_abs_data_T_N], m_data=[m_data_T_1, ...m_data_T_N], where m_abs_data_T_i (m_data_T_i) correspond to the list of measurements taken for temperature T_i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.reset_spins()\n",
    "\n",
    "m_abs_data = []\n",
    "m_data = []\n",
    "\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print(f\"temperature: {temperature}\")\n",
    "    # implement the algorithm here using the class instance sys\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check\n",
    "Let's have a quick look at the order parameter to check if we get results in the same ballpark as our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ops = []\n",
    "for opd in m_abs_data:\n",
    "    ops.append(np.mean(opd))\n",
    "fig= plt.figure(figsize=(9,5))\n",
    "plt.plot(temperatures,ops,'.')\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Order Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = []\n",
    "for md in m_data:\n",
    "    ms.append(np.mean(md))\n",
    "fig= plt.figure(figsize=(9,5))\n",
    "plt.plot(temperatures,ms,'.')\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Magnetization\")\n",
    "plt.ylim([-1.05,1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the magnetization as function of the temperature, you should find that the Metropolis algorithm is trapped in one sector of configuration space at low temperatures (it should be zero for all temperatures). Note that it always ends up in the states with all spins up due to the chosen initial state (you can try to change it to all spins down and see what changes). Even ridiculous amounts of sampling can't rectify this. This highlights the ginormous correlation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Binning Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you use the binning utilities to study the behaviour of the errors vs the level of binning for different temperatures. This gives you an idea about the correlation time. Ideally, you should observe an exponential growth that slows into a plateau (bend the curve!). That's where the money is.\\\\\n",
    "Note that the last few benning level may exhibit finite-size effects, as the number of samples approaches 1. In particular, problems arise close to the critical temperature, where the correlation time diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(int(len(temperatures)/2), 2, figsize=(15,100))\n",
    "for i in range(int(len(temperatures)/2)):\n",
    "    binning_data = binning(m_abs_data[2*i], num_levels)\n",
    "    axs[i,0].plot(binning_data, label=f\"T={temperatures[2*i]}\")\n",
    "    axs[i,0].legend()\n",
    "    axs[i,0].set_ylabel(\"Error\")\n",
    "    binning_data = binning(m_abs_data[2*i+1], num_levels)\n",
    "    axs[i,1].plot(binning_data, label=f\"T={temperatures[2*i+1]}\")\n",
    "    axs[i,1].legend()\n",
    "axs[int(len(temperatures)/2)-1,0].set_xlabel(\"Binning level\");\n",
    "axs[int(len(temperatures)/2)-1,1].set_xlabel(\"Binning level\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the errors, we'll just use the maximum error observed while binning for each temperature. This should provide a rather good (slightly over) estimate for most temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(len(m_abs_data)):\n",
    "    binning_data = binning(m_abs_data[i], num_levels)\n",
    "    errors.append(np.max(binning_data))\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.errorbar(temperatures,ops,yerr=errors, fmt='o', ms=5, ecolor='r', elinewidth=1, capsize=2, barsabove=True)\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Order Parameter\")\n",
    "ax.set_title(\"Ising simulation with the Metropolis algorithm$\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Wolff cluster updates\n",
    "We perform the same steps as in Exercise 1 with the Wolff algorithm. Each step of the Wolff algorithm consists of an iteration procedure to build a cluster of parallel spins with connected sites: At first, a random site $i$ is chosen to seed the cluster. Then, all neighbouring sites that have the same spin as the spin(s) in the cluster are added, each with probability $1-e^{-2\\beta J}$. This is repeated until all boundaries of the cluster have been checked exactly once and no more sites are added. Then, the step is finished by flipping all spins of the cluster and thereby creating a new sample.\n",
    "First of all, we write a class for the cluster updates.\n",
    "In particular, we will need the following member functions:\n",
    "\n",
    "- **update_probabilities**:\n",
    "As in the Metropolis algorithm, we can reduce the computational cost by pre-computing the needed exponentials. Here, we only need to precompute one exponential $e^{-\\beta J}$, which is saved in the member variable self.wolff_prob. Set the value of self.wolff_prob in **update_probabilities**.\n",
    "\n",
    "- **set_temperature**:\n",
    "(Re-)sets the temperature self.T (probability wolff_prob need to be updated when T is set or changed!) \n",
    "\n",
    "- **reset_spins**:\n",
    "Sets the spins (array self.spins) to the initial configuration, which we here choose as all spins up. Keep in mind that we want to calculate the magnetization $\\langle |m| \\rangle=|1.0/L^2 \\langle \\sum_i s_i \\rangle|$. For this, it is useful to keep track of the quantity $M=\\sum_i s_i$ (member variable self.M) during the Metropolis algorithm. Set the initial value of $M$ in **reset_spins**.\n",
    "\n",
    "- **wolff_step**: \n",
    "Performs one step of the wolff algorithm. Keep in mind to update self.M as well.\n",
    "\n",
    "- **wolff_sweep**:\n",
    "Performs one Monte Carlo sweep, consisting of $L*L$ steps.\n",
    "\n",
    "*Hint (optional): It can be useful (with respect to the computational time) to store the indices of sites at the cluster boundaries with parallel spin using queue.Queue()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    '''utility class to make to wolff marginally more legible. holds an index pair. (Usage optionally) '''\n",
    "    def __init__(self, i, j):\n",
    "        self.i = i\n",
    "        self.j = j\n",
    "        \n",
    "class IsingMC_Wolff:\n",
    "    def __init__(self, length, temperature=0.):\n",
    "        self.spins = np.ones((length,length),dtype=int)\n",
    "        self.L = length\n",
    "        self.T = temperature\n",
    "        self.M = length * length #we start with all spins up\n",
    "        self.wolff_prob = None\n",
    "        self.wolff_marker = np.zeros((length,length),dtype=int) #container to mark which sites are in the cluster\n",
    "        self.update_probabilities()\n",
    "    \n",
    "    def update_probabilities(self):\n",
    "        '''we calculate the probability in the beginning so we don't have to recompute it'''\n",
    "        if(self.T != 0.):\n",
    "            #wolff acceptance probability\n",
    "            #implement here\n",
    "            pass\n",
    "           \n",
    "        else:\n",
    "            #wolff acceptance probability\n",
    "            #implement here\n",
    "            pass\n",
    "            \n",
    "    def set_temperature(self, temperature):\n",
    "        '''set temperature and update the probability'''\n",
    "        #implement here  \n",
    "    \n",
    "    \n",
    "    def reset_spins(self):\n",
    "        '''this resets the spins to the all-up state '''\n",
    "        #implement here\n",
    "\n",
    "    \n",
    "    def wolff_step(self):\n",
    "        '''perform one update step using wolff'''\n",
    "        #implement wolff step here\n",
    "    \n",
    "    def wolff_sweep(self):\n",
    "        '''perform an update sweep using wolff '''\n",
    "        #implement sweep here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Relaxation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "dt = 0.1\n",
    "num_updates = L*L*10\n",
    "\n",
    "sys = IsingMC_Wolff(L)\n",
    "temperatures = np.arange(0.,5.0,dt)\n",
    "data = []\n",
    "for t in temperatures:\n",
    "    mag_data = []\n",
    "    sys.reset_spins()\n",
    "    sys.set_temperature(t)\n",
    "    for update in range(num_updates):\n",
    "        sys.wolff_step()\n",
    "        mag_data.append(np.abs(sys.M)/(sys.L*sys.L))\n",
    "    data.append(mag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating averages...\")\n",
    "averages = []\n",
    "count = 0\n",
    "for dataset in data:\n",
    "    current = []\n",
    "    for i in range(1,len(dataset),10):\n",
    "        current.append(np.mean(dataset[:i]))\n",
    "    averages.append(np.array(current))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(25, 2, figsize=(15,69))\n",
    "for i in range(25):\n",
    "    axs[i,0].plot(averages[2*i], label=f\"T={temperatures[2*i]}\")\n",
    "    axs[i,0].legend()\n",
    "    axs[i,0].set_ylim([0.,1.05])\n",
    "    axs[i,0].set_ylabel(\"Order parameter estimate\")    \n",
    "    axs[i,0].set_xlabel(\"Number of sweeps x 10\")\n",
    "    axs[i,1].plot(averages[2*i+1], label=f\"T={temperatures[2*i+1]}\")\n",
    "    axs[i,1].legend()\n",
    "    axs[i,1].set_ylim([0.,1.05])    \n",
    "    axs[i,1].set_xlabel(\"Number of sweeps x 10\")\n",
    "\n",
    "axs[24,0].set_xlabel(\"Average cutoff (step number)\");\n",
    "axs[24,1].set_xlabel(\"Average cutoff (step number)\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "You should observe that, with the set parameters, after 150 steps we are typically well relaxed. For temperatures < 2, 30 steps seem to do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Note that we take far fewer samples than with the Metropolis algorithm. However, we take different numbers of samples for different temperatures. We do this because the cluster sizes shrink as temperature rises.\n",
    "\n",
    "We take many more samples than we actually need. I ran this with half the samples before, and still got better results than with the Metropolis algorithm. Hence if you don't feel like waiting longer than necessary, you can safely halve each num_samples, and lower the num_values by 1 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temperatures at which we sample\n",
    "#finer graining around the critical temperature; this is where the money is!\n",
    "temps_low = np.arange(0.,2.1,0.1)\n",
    "temps_crit = np.arange(2.1,2.7,0.02)\n",
    "temps_high = np.arange(2.8,5.,0.1)\n",
    "temperatures = np.concatenate((temps_low,temps_crit,temps_high))\n",
    "\n",
    "#number of relaxation steps we want to perform\n",
    "def relaxation_steps(temperature):\n",
    "    if temperature < 2:\n",
    "        return 30\n",
    "    else:\n",
    "        return 150\n",
    "\n",
    "#we take an adaptive number of samples\n",
    "#it takes some playing around to find out what makes sense at which temperature\n",
    "#note that the cluster size goes down with rising temperature\n",
    "def num_samples(temperature):\n",
    "    if temperature < 1.3:\n",
    "        return 2**9\n",
    "    elif temperature < 2.4:\n",
    "        return 2**13\n",
    "    elif temperature < 3.:\n",
    "        return 2**14\n",
    "    else:\n",
    "        return 2**15\n",
    "\n",
    "#thus also the maximum binning levels need to be adaptive\n",
    "def num_levels(temperature):\n",
    "    if temperature < 1.3:\n",
    "        return 8\n",
    "    elif temperature < 2.4:\n",
    "        return 11\n",
    "    elif temperature < 3.:\n",
    "        return 12\n",
    "    else:\n",
    "        return 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "Implement here the Wolff algorithm for each temperature including relaxation sweeps in order to equilibrate the system. After thermalization, collect the magnetization and the absolute value of the magnetization. Save the complete collection of the measurements of $|m|$ and $m$ in \n",
    "m_abs_data=[m_abs_data_T_1, ...m_abs_data_T_N], m_data=[m_data_T_1, ...m_data_T_N], where m_abs_data_T_i (m_data_T_i) correspond to the list of measurements taken for temperature T_i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.reset_spins()\n",
    "\n",
    "m_abs_data = []\n",
    "m_data = []\n",
    "for temperature in temperatures:\n",
    "    print(f\"temperature: {temperature}\")\n",
    "    #Implement the Wolff algorithm here using the instance sys of the class IsingMC_Wolff\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ops = []\n",
    "for opd in m_abs_data:\n",
    "    ops.append(np.mean(opd))\n",
    "fig= plt.figure(figsize=(9,5))\n",
    "plt.plot(temperatures,ops,'.')\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Order Parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = []\n",
    "for md in m_data:\n",
    "    ms.append(np.mean(md))\n",
    "fig= plt.figure(figsize=(9,5))\n",
    "plt.plot(temperatures,ms,'.')\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Magnetization\")\n",
    "plt.ylim([-1.05,1.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnetization shows that we are not stuck in one region of configuration space. The correlation time is drastically reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Binning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(int(len(temperatures)/2), 2, figsize=(15,100))\n",
    "for i in range(int(len(temperatures)/2)):\n",
    "    binning_data = binning(op_data[2*i], num_levels(temperatures[2*i]))\n",
    "    axs[i,0].plot(binning_data, label=f\"T={temperatures[2*i]}\")\n",
    "    axs[i,0].legend()\n",
    "    axs[i,0].set_ylabel(\"Error\")\n",
    "    binning_data = binning(op_data[2*i+1], num_levels(temperatures[2*i+1]))\n",
    "    axs[i,1].plot(binning_data, label=f\"T={temperatures[2*i+1]}\")\n",
    "    axs[i,1].legend()\n",
    "axs[int(len(temperatures)/2)-1,0].set_xlabel(\"Binning level\");\n",
    "axs[int(len(temperatures)/2)-1,1].set_xlabel(\"Binning level\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of these look converged. Note that we also get erratic behaviour due to finite size effects. We look for exponential growth that flattens. We see that these curves plateau much quicker than for M(RT)^2, highlighting again that the sampling is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(len(m_abs_data)):\n",
    "    binning_data = binning(m_abs_data[i], num_levels(temperatures[i]))\n",
    "    errors.append(np.max(binning_data))\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.errorbar(temperatures,ops,yerr=errors, fmt='o', ms=5, ecolor='r', elinewidth=1, capsize=2, barsabove=True)\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Order Parameter\")\n",
    "ax.set_title(\"Ising simulation with Wolff cluster updates\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: 2D Ising model on triangular lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will build upon your existing implementation of the 2D square lattice Ising\n",
    "model and adapt it to simulate the Ising model on a triangular lattice. Instead of starting from\n",
    "scratch, you can exploit the capabilities of the large language model (ChatGPT) to guide you\n",
    "through the necessary modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: identify the necessary changes\n",
    "Due to the different geometry you would need to modify\n",
    "- the lattice geometry;\n",
    "- the nearest neighbor interactions;\n",
    "- the implementation of the boundary conditions (we work with p.b.c like for the 2D square model).\n",
    "\n",
    "You can ask ChatGPT to apply the necessary changes to your existing code.\n",
    "\n",
    "Review the changes to be sure that they agree with your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: simulation\n",
    "\n",
    "Use the same utilities as for the square lattice case to run the simulations at different temperatures. Explore the range $T \\in [0,5]$.\n",
    "Before running the actual simulations identify the number of relaxation sweeps needed at each temperature. Consider that the expected critical temperature for the triangular Ising model is $3.641J/k_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: analysis\n",
    "\n",
    "Plot the results for the magnetization as function of the temperature and roughly identify the critical temperature to check the correctness of the implementation. Is the value compatible with the expectation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_CQM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
